---
path: /home/analyzing-experiments
title: Analyzing experiments
description: This topic explains how to interpret an experiment's results and apply its findings to your product.
published: true
tags: ['analyze', 'result', 'experiment', 'experimentation', 'enterprise']
---

## Overview

This topic explains how to interpret an experiment's results and apply its findings to your product.

## Understanding experiments as they run

When your experiments are running, you can view information about them on the **Experiments** list or on the related flag's **Experimentation** tab. The **Experimentation** tab displays all the experiments a flag is participating in, including both experiments that are currently recording and experiments that are stopped.

Here are some things you can do with each experiment:

- Stop the experiment or start a new iteration. To learn more, read [Managing experiments](/home/analyzing-experiments/managing).
- Edit the metrics connected to the experiment and start a new iteration.
- View experiment data over set periods of time on the **Iterations** tab:

![An experiment's "Iterations" tab.](experiment-iterations-tab.png)

## Reading experiment data

The data an experiment has collected is represented in a results graph and table:

![An experiment's results graph and table.](experiment-results-table-graph.png)

The horizontal x-axis displays the unit of the primary metric included in the experiment. For example, if the metric is measuring revenue, the unit might be dollars, or if the metric is measuring website latency, the unit might be milliseconds. The vertical y-axis measures probability.

To learn more about interpreting an experiment's results, read [Reading results graphs](/home/analyzing-experiments/results).

## Determining how long an experiment should run

When conducting an experiment, two factors are crucial in deciding when to make a decision:

1. The current probability of the winning variation being the best, and how long it would take to improve your confidence.

2. The level of risk involved in rolling out the winning option to all users.

By both analyzing the results of the sample size estimator and weighing the risk of having possible negative impact on a metric, you can determine if it's worth waiting for more data to increase confidence or if the risk is low enough to proceed. Figuring out the risk of decision can be done by looking at the metrics being experimented on, and determining the business value that metric represents. Continuing to run an experiment with a low probability of positive impact and a high risk of negative impact would not make sense, for example.

![An experiments sample size estimator results.](sample-size-estimator.png)

## Choosing a winning variation

The winning variation for a completed experiment is the variation that is most likely to be the best option out of all of the variations tested. To learn more, read [Winning variations](/home/analyzing-experiments/winning-variation).

<Callout intent="info">
<CalloutTitle>Consider stopping an experiment after you choose a winning variation</CalloutTitle>
<CalloutDescription>

If you're done with an experiment and have rolled out the winning variation to your user base, it is a good time to stop your experiment. Experiments running on a user base that only receives one flag variation do not return useful results. Stopping an experiment retains all the data collected so far.

</CalloutDescription>
</Callout>

## Further analyzing results

If you're using Data Export, you can find experiment data in your Data Export destinations to further analyze it using third-party tools of your own.

To learn more, read [Data Export](/home/data-export).
